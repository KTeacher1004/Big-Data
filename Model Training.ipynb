{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31bc8f87",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d9a9c843",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os, re, html, math, random, time\n",
    "import numpy as np, pymongo, torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0543e428",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: NVIDIA GeForce RTX 3060\n"
     ]
    }
   ],
   "source": [
    "for k in (\"CUDA_VISIBLE_DEVICES\", \"CUDA_DEVICE_ORDER\"):\n",
    "    os.environ.pop(k, None)\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using device:\", torch.cuda.get_device_name(0) if DEVICE==\"cuda\" else \"CPU\")\n",
    "if DEVICE == \"cuda\":\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "39b132ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "MONGO_URI = os.getenv(\n",
    "    \"MONGO_URI\",\n",
    "    \"mongodb+srv://admin:adminpassword@assignment3.dhfn7vh.mongodb.net/?retryWrites=true&w=majority&appName=Assignment3\"\n",
    ")\n",
    "DB_NAME   = \"Assignment3\"\n",
    "COLL_NAME = \"static_reviews\"\n",
    "\n",
    "MODEL_NAME = \"nlptown/bert-base-multilingual-uncased-sentiment\"   # outputs 1–5★\n",
    "BATCH_SZ   = 256 if DEVICE==\"cuda\" else 32\n",
    "MAX_LEN    = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d8b2928b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(s):\n",
    "    if not s: return \"\"\n",
    "    s = html.unescape(s)\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s\n",
    "\n",
    "def load_mongo(limit=None, sample_every=None):\n",
    "    client = pymongo.MongoClient(MONGO_URI, serverSelectionTimeoutMS=5000, socketTimeoutMS=120000)\n",
    "    coll = client[DB_NAME][COLL_NAME]\n",
    "\n",
    "    q = {\"rating\": {\"$exists\": True}}\n",
    "    proj = {\"text\": 1, \"title\": 1, \"rating\": 1, \"asin\": 1, \"parent_asin\": 1, \"helpful_vote\": 1, \"verified_purchase\": 1}\n",
    "\n",
    "    cursor = coll.find(q, proj, batch_size=5000)\n",
    "    if limit:\n",
    "        cursor = cursor.limit(int(limit))\n",
    "\n",
    "    X, y, groups, weights = [], [], [], []\n",
    "    i = 0\n",
    "    try:\n",
    "        for d in cursor:\n",
    "            i += 1\n",
    "            if sample_every and (i % int(sample_every) != 0):\n",
    "                continue\n",
    "\n",
    "            # 1) title + text with delimiter\n",
    "            title = d.get(\"title\") or \"\"\n",
    "            text  = d.get(\"text\") or \"\"\n",
    "            if not isinstance(title, str): title = str(title) if title is not None else \"\"\n",
    "            if not isinstance(text, str):  text  = str(text)  if text  is not None else \"\"\n",
    "            combined = \" [SEP] \".join(t for t in (clean_text(title), clean_text(text)) if t).strip()\n",
    "\n",
    "            # 2) skip degenerate rows\n",
    "            if len(combined) < 3:\n",
    "                continue\n",
    "\n",
    "            # 3) rating clamp\n",
    "            try:\n",
    "                r = float(d[\"rating\"])\n",
    "            except Exception:\n",
    "                continue\n",
    "            r = max(1.0, min(5.0, r))\n",
    "\n",
    "            # 4) group by asin (fallback to parent_asin or small buckets)\n",
    "            asin = d.get(\"asin\") or d.get(\"parent_asin\")\n",
    "            if not asin:\n",
    "                asin = f\"_nogroup_{(len(X))//100}\"\n",
    "\n",
    "            # 5) optional weights\n",
    "            hv = d.get(\"helpful_vote\", 0) or 0\n",
    "            vp = 1.2 if d.get(\"verified_purchase\") else 1.0\n",
    "            w = vp * (1.0 + min(float(hv), 50.0) / 50.0)  # cap helpfulness to avoid extremes\n",
    "\n",
    "            X.append(combined)\n",
    "            y.append(r)\n",
    "            groups.append(str(asin))\n",
    "            weights.append(w)\n",
    "    finally:\n",
    "        try: cursor.close()\n",
    "        except: pass\n",
    "\n",
    "    return X, np.array(y, dtype=np.float32), np.array(groups), np.array(weights, dtype=np.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2578100c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def eval_metrics(y_true, y_pred, detailed=False):\n",
    "\n",
    "    y_true = np.array(y_true, dtype=np.float32)\n",
    "    y_pred = np.clip(np.array(y_pred, dtype=np.float32), 1.0, 5.0)\n",
    "\n",
    "    err = np.abs(y_true - y_pred)\n",
    "\n",
    "    mae  = float(np.mean(err))\n",
    "    rmse = float(np.sqrt(np.mean((y_true - y_pred) ** 2)))\n",
    "    within_05 = float(np.mean(err <= 0.5))\n",
    "    within_10 = float(np.mean(err <= 1.0))\n",
    "\n",
    "    # R²\n",
    "    ss_res = float(np.sum((y_true - y_pred) ** 2))\n",
    "    ss_tot = float(np.sum((y_true - np.mean(y_true)) ** 2))\n",
    "    r2 = 1 - ss_res / ss_tot if ss_tot > 0 else float(\"nan\")\n",
    "\n",
    "    results = {\n",
    "        \"MAE\": mae,\n",
    "        \"RMSE\": rmse,\n",
    "        \"Within_0.5\": within_05,\n",
    "        \"Within_1.0\": within_10,\n",
    "        \"R2\": r2,\n",
    "    }\n",
    "\n",
    "    if detailed:\n",
    "        df = pd.DataFrame({\"true\": y_true, \"err\": err})\n",
    "        results[\"PerStarMAE\"] = (\n",
    "            df.groupby(np.rint(df[\"true\"]).astype(int))[\"err\"].mean().to_dict()\n",
    "        )\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2b162901",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id2label mapping: {0: '1 star', 1: '2 stars', 2: '3 stars', 3: '4 stars', 4: '5 stars'}\n",
      "Stars tensor: [1.0, 2.0, 3.0, 4.0, 5.0]\n"
     ]
    }
   ],
   "source": [
    "def stars_tensor_from_id2label(model, device):\n",
    "\n",
    "    id2label = getattr(model.config, \"id2label\", {})\n",
    "    stars = []\n",
    "    for i in range(model.config.num_labels):\n",
    "        lbl = str(id2label.get(i, f\"LABEL_{i}\")).lower().strip()\n",
    "        # match \"1 star\" or \"5 stars\"\n",
    "        m = re.match(r\"^\\s*(\\d+)\\s*stars?$\", lbl)\n",
    "        if m:\n",
    "            stars.append(int(m.group(1)))\n",
    "        else:\n",
    "            # fallback for \"LABEL_0\" → 1, etc.\n",
    "            m2 = re.match(r\"^\\s*label[_\\s-]?(\\d+)\\s*$\", lbl)\n",
    "            stars.append(int(m2.group(1)) + 1 if m2 else (i + 1))\n",
    "\n",
    "    # sanity check\n",
    "    if len(set(stars)) != model.config.num_labels:\n",
    "        raise ValueError(f\"Unexpected star mapping: {id2label} -> {stars}\")\n",
    "\n",
    "    stars_tensor = torch.tensor(stars, dtype=torch.float32, device=device)\n",
    "    print(\"id2label mapping:\", id2label)\n",
    "    print(\"Stars tensor:\", stars_tensor.tolist())\n",
    "    return stars_tensor\n",
    "\n",
    "# Load model + tokenizer\n",
    "tok = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME).to(DEVICE).eval()\n",
    "\n",
    "# Use FP16 on GPU for speed\n",
    "if DEVICE == \"cuda\":\n",
    "    model = model.half()\n",
    "\n",
    "KS = stars_tensor_from_id2label(model, DEVICE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d690023",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_expected_stars(texts, batch_size=BATCH_SZ, max_length=MAX_LEN, device=DEVICE):\n",
    "    preds = []\n",
    "    model.eval()  # make sure we're in eval mode\n",
    "\n",
    "    # Inference context\n",
    "    ctx = torch.inference_mode()\n",
    "    autocast_ctx = (\n",
    "        torch.cuda.amp.autocast(dtype=torch.float16)\n",
    "        if device.startswith(\"cuda\")\n",
    "        else torch.no_grad()\n",
    "    )\n",
    "\n",
    "    with ctx:\n",
    "        with autocast_ctx:\n",
    "            for i in tqdm(\n",
    "                range(0, len(texts), batch_size),\n",
    "                total=(len(texts) + batch_size - 1) // batch_size,\n",
    "                desc=\"Infer\",\n",
    "            ):\n",
    "                chunk = texts[i:i + batch_size]\n",
    "\n",
    "                # Defensive: coerce to non-empty strings\n",
    "                chunk = [str(t) if isinstance(t, str) else \".\" for t in chunk]\n",
    "                chunk = [c if c.strip() else \".\" for c in chunk]\n",
    "\n",
    "                enc = tok(\n",
    "                    chunk,\n",
    "                    return_tensors=\"pt\",\n",
    "                    padding=True,\n",
    "                    truncation=True,\n",
    "                    max_length=max_length,\n",
    "                )\n",
    "                enc = {k: v.to(device) for k, v in enc.items()}\n",
    "\n",
    "                logits = model(**enc).logits.float() \n",
    "                probs = torch.softmax(logits, dim=-1)       \n",
    "                exp = (probs * KS).sum(dim=1).cpu().numpy() \n",
    "                preds.extend(exp.tolist())\n",
    "\n",
    "    return preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b18fabda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 100000 examples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bazen\\AppData\\Local\\Temp\\ipykernel_20008\\1567493457.py:8: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  torch.cuda.amp.autocast(dtype=torch.float16)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd18799dbaef45d5bfd86bc962d33d64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Infer:   0%|          | 0/391 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time: 138.3s  |  ~723.2 reviews/sec\n",
      "nlptown metrics: {'MAE': 0.469960480928421, 'RMSE': 0.6844781041145325, 'Within_0.5': 0.65358, 'Within_1.0': 0.89259, 'R2': 0.7655525602169655}\n",
      "\n",
      "Examples:\n",
      "- true=5.0  pred=4.84  |  Amazing value [SEP] Fantastic sound for the money, great for everyday listening.\n",
      "- true=5.0  pred=4.73  |  coolstream duo review [SEP] This adapter created a better way of listening to music from my smartphone. Made me put away the cd's. Now I enjoy services like Pan\n",
      "- true=5.0  pred=4.99  |  Five Stars [SEP] Awesome Modem\n",
      "- true=5.0  pred=4.59  |  Great quality! Worth the buy! [SEP] I purchased these for my son's headphones to replace the originals. Was extremely pleased with the quality of these replacem\n",
      "- true=5.0  pred=4.87  |  Love these speakers [SEP] Love, love love don't let the size fool you they pack a powerful punch. suburb quality\n"
     ]
    }
   ],
   "source": [
    "X, y, groups, weights = load_mongo()\n",
    "print(f\"Loaded {len(X)} examples\")\n",
    "t0 = time.time()\n",
    "yhat = predict_expected_stars(X)\n",
    "if DEVICE == \"cuda\":\n",
    "    torch.cuda.synchronize()\n",
    "t = time.time() - t0\n",
    "print(f\"Total time: {t:.1f}s  |  ~{len(X)/max(t,1e-6):.1f} reviews/sec\")\n",
    "\n",
    "print(\"nlptown metrics:\", eval_metrics(y, yhat))\n",
    "\n",
    "# Sample outputs\n",
    "print(\"\\nExamples:\")\n",
    "for idx in random.sample(range(len(X)), k=min(5, len(X))):\n",
    "    print(f\"- true={y[idx]:.1f}  pred={yhat[idx]:.2f}  |  {X[idx][:160]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b0337946",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model + tokenizer to nlptown_model\n"
     ]
    }
   ],
   "source": [
    "import joblib, os\n",
    "\n",
    "SAVE_DIR = \"nlptown_model\"\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "# 1. Save Hugging Face model + tokenizer (reloadable with from_pretrained)\n",
    "model.save_pretrained(SAVE_DIR)\n",
    "tok.save_pretrained(SAVE_DIR)\n",
    "\n",
    "# 2. Save your stars mapping tensor (so you don’t recompute)\n",
    "joblib.dump(KS.cpu().numpy(), os.path.join(SAVE_DIR, \"stars_tensor.joblib\"))\n",
    "\n",
    "print(f\"Saved model + tokenizer to {SAVE_DIR}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
